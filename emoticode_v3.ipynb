{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/emoticode/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding Logic Ready (LaBSE).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: LaBSE Embedding Generation ---\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configuration\n",
    "# LaBSE output dimension is 768\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/LaBSE\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# LaBSE is truly multilingual, so we don't need strict language codes like 'kas_Arab'.\n",
    "# It detects script/language features automatically from the text.\n",
    "LANG_FILTER = ['Kashmiri', 'Santali', 'Manipuri']\n",
    "\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def get_embeddings_managed(df, cache_path, is_train=True):\n",
    "    # 1. Check Cache\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"ðŸ“‚ Found cached embeddings at {cache_path}. Loading...\")\n",
    "        return torch.load(cache_path, map_location='cpu')\n",
    "\n",
    "    print(f\"âš¡ Generating LaBSE embeddings for {len(df)} samples...\")\n",
    "    \n",
    "    # 2. Load Model (Inside function to allow deletion later)\n",
    "    print(\"   -> Loading LaBSE Model...\")\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
    "    \n",
    "    # 3. Filter Data\n",
    "    if is_train:\n",
    "        # Keep only the target languages\n",
    "        df = df[df['language'].isin(LANG_FILTER)]\n",
    "    \n",
    "    # 4. Generate Embeddings\n",
    "    sentences = df['Sentence'].tolist()\n",
    "    \n",
    "    # Model.encode handles batching and GPU memory automatically!\n",
    "    # show_progress_bar=True gives you a nice visual\n",
    "    print(\"   -> Encoding sentences (Batching handled automatically)...\")\n",
    "    embeddings_numpy = model.encode(\n",
    "        sentences, \n",
    "        batch_size=64, \n",
    "        show_progress_bar=True, \n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True # LaBSE works best with normalized vectors\n",
    "    )\n",
    "    \n",
    "    # 5. Save\n",
    "    final_tensor = torch.tensor(embeddings_numpy)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saving to {cache_path}...\")\n",
    "    torch.save(final_tensor, cache_path)\n",
    "    \n",
    "    # 6. Cleanup\n",
    "    del model\n",
    "    cleanup_memory()\n",
    "    print(\"   -> Model unloaded. Memory cleared.\")\n",
    "    \n",
    "    return final_tensor\n",
    "\n",
    "print(\"âœ… Embedding Logic Ready (LaBSE).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Class Ready (768-Dim Input).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: LaBSE-Gemma Classifier Definition ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "class LaBSEGemmaClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_labels=6, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # 1. Load Gemma (4-bit)\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        print(\"   -> Loading Gemma-3-1B Backbone...\")\n",
    "        self.gemma_backbone = AutoModel.from_pretrained(\n",
    "            \"google/gemma-3-1b-it\",\n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": device},\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 2. Stability Fixes\n",
    "        self.gemma_backbone = prepare_model_for_kbit_training(self.gemma_backbone)\n",
    "        \n",
    "        # 3. LoRA Adapters\n",
    "        peft_config = LoraConfig(\n",
    "            r=16, \n",
    "            lora_alpha=32, \n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], \n",
    "            lora_dropout=0.05, \n",
    "            bias=\"none\", \n",
    "            task_type=TaskType.FEATURE_EXTRACTION \n",
    "        )\n",
    "        self.gemma_backbone = get_peft_model(self.gemma_backbone, peft_config)\n",
    "        \n",
    "        # 4. Projector (LaBSE 768 -> Gemma Hidden)\n",
    "        self.hidden_size = self.gemma_backbone.config.hidden_size \n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        ).to(device)\n",
    "        \n",
    "        # 5. Head\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_labels).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        # embeddings: [Batch, 768]\n",
    "        projected = self.projector(embeddings)     # -> [Batch, Gemma_Dim]\n",
    "        inputs_embeds = projected.unsqueeze(1)     # -> [Batch, 1, Gemma_Dim]\n",
    "        \n",
    "        outputs = self.gemma_backbone(inputs_embeds=inputs_embeds)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "print(\"âœ… Model Class Ready (768-Dim Input).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING LaBSE + GEMMA PIPELINE ===\n",
      "âš¡ Generating LaBSE embeddings for 7176 samples...\n",
      "   -> Loading LaBSE Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Encoding sentences (Batching handled automatically)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:09<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving to train_labse.pt...\n",
      "   -> Model unloaded. Memory cleared.\n",
      "âš¡ Generating LaBSE embeddings for 2392 samples...\n",
      "   -> Loading LaBSE Model...\n",
      "   -> Encoding sentences (Batching handled automatically)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:02<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving to test_labse.pt...\n",
      "   -> Model unloaded. Memory cleared.\n",
      "\n",
      "=== Initializing Gemma Classifier ===\n",
      "   -> Loading Gemma-3-1B Backbone...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Main Execution ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Mappings\n",
    "EMOTION_MAP = {'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4, 'surprise': 5}\n",
    "ID_TO_EMOTION = {v: k for k, v in EMOTION_MAP.items()}\n",
    "LANG_FILTER = ['Kashmiri', 'Santali', 'Manipuri']\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def main():\n",
    "    print(\"=== STARTING LaBSE + GEMMA PIPELINE ===\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df_train = pd.read_csv(\"dataset/competition_train.csv\")\n",
    "        df_test = pd.read_csv(\"dataset/competition_test.csv\")\n",
    "    except FileNotFoundError:\n",
    "        # Fallback for local testing if needed\n",
    "        df_train = pd.read_csv(\"competition_train.csv\")\n",
    "        df_test = pd.read_csv(\"competition_test.csv\")\n",
    "\n",
    "    # 2. Get Embeddings (LaBSE)\n",
    "    # Note: is_train=True filters the dataset to only include valid languages\n",
    "    X_all = get_embeddings_managed(df_train, \"train_labse.pt\", is_train=True)\n",
    "    X_test = get_embeddings_managed(df_test, \"test_labse.pt\", is_train=False)\n",
    "    \n",
    "    # Ensure Memory is Clean\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # 3. Prepare Training Data\n",
    "    # We must filter df_train to match the size of X_all\n",
    "    df_train_clean = df_train[df_train['language'].isin(LANG_FILTER)].copy()\n",
    "    y_all = torch.tensor(df_train_clean['emotion'].map(EMOTION_MAP).values, dtype=torch.long)\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.15, random_state=42, stratify=y_all)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "    \n",
    "    # 4. Initialize Classifier\n",
    "    print(\"\\n=== Initializing Gemma Classifier ===\")\n",
    "    # Input dim is 768 for LaBSE\n",
    "    model = LaBSEGemmaClassifier(input_dim=768, num_labels=6, device=DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4)\n",
    "    \n",
    "    # 5. Training Loop\n",
    "    EPOCHS = 5\n",
    "    print(f\"\\n=== Training for {EPOCHS} Epochs ===\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(batch_X, labels=batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(DEVICE)\n",
    "                logits, _ = model(batch_X)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_targets.extend(batch_y.numpy())\n",
    "        \n",
    "        acc = accuracy_score(val_targets, val_preds)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "    # 6. Generate Submission\n",
    "    print(\"\\n=== Generating Submission ===\")\n",
    "    test_loader = DataLoader(TensorDataset(X_test), batch_size=32)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(test_loader, desc=\"Predicting\"):\n",
    "            batch_X = batch[0].to(DEVICE)\n",
    "            logits, _ = model(batch_X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': df_test['id'],\n",
    "        'emotion': [ID_TO_EMOTION[p] for p in all_preds]\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nâœ… Success! 'submission.csv' is ready.\")\n",
    "\n",
    "# Run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
